{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giotaChita/Artificial-Intelligence/blob/main/exe5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJpugoTtk5nb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import csv\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trainy=pd.read_csv('y_train.csv')\n",
        "train=pd.read_csv('X_train.csv')\n",
        "print(train)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Mm1NQpxlgBT",
        "outputId": "be3dd7ac-fa2c-458b-ef86-f50d6a0c264e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
            "0           0       0       0       0       0       0       0       0       0   \n",
            "1           0       0       0       0       0       0       0       0       0   \n",
            "2           0       0       0       0       0       0       0       0       0   \n",
            "3           0       0       0       0       0       0       0       0       0   \n",
            "4           0       0       0       0       0       0       0       0       0   \n",
            "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
            "37928       0       0       0       0       0       0       0       0       0   \n",
            "37929       0       0       0       0       0       0       0       0       0   \n",
            "37930       0       0       0       0       0       0       0       0       0   \n",
            "37931       3      11       0       0       0       0       0       0       0   \n",
            "37932       0       0       0       0       0       0       0       0       0   \n",
            "\n",
            "       pixel9  ...  pixel1014  pixel1015  pixel1016  pixel1017  pixel1018  \\\n",
            "0           0  ...          0          0          0          0          0   \n",
            "1           0  ...          0          0          0          0          0   \n",
            "2           0  ...          0          0          0          0          0   \n",
            "3           0  ...          0          0          0          0          0   \n",
            "4           0  ...          0          0          0          0          0   \n",
            "...       ...  ...        ...        ...        ...        ...        ...   \n",
            "37928       0  ...          0          0          0          0          0   \n",
            "37929       0  ...          0          0          0          0          0   \n",
            "37930       0  ...          0          0          0          0          0   \n",
            "37931       0  ...          0          0          0          0          0   \n",
            "37932       0  ...          0          0          0          0          0   \n",
            "\n",
            "       pixel1019  pixel1020  pixel1021  pixel1022  pixel1023  \n",
            "0              0          0          0          4          6  \n",
            "1              0          0          0          0          0  \n",
            "2              0          0          0          0          0  \n",
            "3              0          0          0          0          0  \n",
            "4              0          0          0          0          0  \n",
            "...          ...        ...        ...        ...        ...  \n",
            "37928          0          0          0          0          0  \n",
            "37929          0          0          0          0          0  \n",
            "37930          0          0          0          0          0  \n",
            "37931          0          0          0          0          0  \n",
            "37932          0          0          0          0          0  \n",
            "\n",
            "[37933 rows x 1024 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.insert(1024, \"label\", list(trainy.label.values-1), True)\n",
        "np.random.shuffle(train.values)\n",
        "trainx=train.copy()\n",
        "\n",
        "del trainx[\"label\"]\n",
        "sc = StandardScaler()\n",
        "trainx = sc.fit_transform(trainx)\n",
        "trainy=train.loc[:,\"label\"]\n",
        "print(trainy)\n",
        "\n"
      ],
      "metadata": {
        "id": "9N1XXUly9tQk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61368bc9-eb93-45e4-a01d-134526c0c489"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0         0\n",
            "1        22\n",
            "2        24\n",
            "3        20\n",
            "4        13\n",
            "         ..\n",
            "37928    28\n",
            "37929     6\n",
            "37930    12\n",
            "37931    28\n",
            "37932    21\n",
            "Name: label, Length: 37933, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainy=np.asarray(trainy)\n",
        "ohe = OneHotEncoder(sparse=False)\n",
        "trainy=trainy.reshape(len(trainy),1)\n",
        "trainy = ohe.fit_transform(trainy)\n",
        "print(trainy)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(trainx,trainy, test_size=0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVM-WLLeI_fP",
        "outputId": "18fe6889-d7b8-4825-e345-2475124df471"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lay=int(input(\"How many layers do you want the network to have (min 2)? \"))\n",
        "methodopt=int(input(\"Which hyperparameter optimization method to use? (1 for seperate, 2 for random) \"))\n",
        "bestact=''\n",
        "bestunits=[50 for k in range(lay)]\n",
        "bestacc=0\n",
        "if methodopt==1:\n",
        "\n",
        "  act=('relu','sigmoid','tanh')\n",
        "\n",
        "  for j in act:\n",
        "    model=keras.models.Sequential()\n",
        "\n",
        "    model.add(keras.layers.Flatten(input_dim=(1024)))\n",
        "\n",
        "    for i in range((lay-2)):\n",
        "        model.add(keras.layers.Dense(units=50,activation=j))\n",
        "\n",
        "    model.add(keras.layers.Dense(units=29,activation='sigmoid'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy','mse','mae'])\n",
        "\n",
        "    model.fit(X_train,Y_train,epochs=20,batch_size=200)\n",
        "\n",
        "    y_pred=model.predict(X_test)\n",
        "    pred = list()\n",
        "    for i in range(len(y_pred)):\n",
        "        pred.append(np.argmax(y_pred[i]))\n",
        "    test = list()\n",
        "    for i in range(len(Y_test)):\n",
        "        test.append(np.argmax(Y_test[i]))\n",
        "    acc=accuracy_score(test,pred)\n",
        "    if acc>bestacc:\n",
        "      bestacc=acc\n",
        "      bestact=j\n",
        "\n",
        "\n",
        "  nounits=(100,150,200,250,300)\n",
        "  for j in nounits:\n",
        "    model=keras.models.Sequential()\n",
        "\n",
        "    model.add(keras.layers.Flatten(input_dim=(1024)))\n",
        "\n",
        "    for i in range((lay-2)):\n",
        "        model.add(keras.layers.Dense(units=j,activation=bestact))\n",
        "\n",
        "    model.add(keras.layers.Dense(units=29,activation='sigmoid'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy','mse','mae'])\n",
        "\n",
        "    model.fit(X_train,Y_train,epochs=20,batch_size=200)\n",
        "\n",
        "    y_pred=model.predict(X_test)\n",
        "    pred = list()\n",
        "    for i in range(len(y_pred)):\n",
        "        pred.append(np.argmax(y_pred[i]))\n",
        "    test = list()\n",
        "    for i in range(len(Y_test)):\n",
        "        test.append(np.argmax(Y_test[i]))\n",
        "    acc=accuracy_score(test,pred)\n",
        "    if acc>bestacc:\n",
        "      bestacc=acc\n",
        "      bestunits=[j for k in range(lay)]\n",
        "\n",
        "\n",
        "else:\n",
        "  act=['relu','sigmoid','tanh']\n",
        "  for rnt in range(100):\n",
        "    tact=act[random.randrange(0,3)]\n",
        "    nounits=[random.randrange(50,300) for k in range(lay-2)]\n",
        "\n",
        "    model=keras.models.Sequential()\n",
        "\n",
        "    model.add(keras.layers.Flatten(input_dim=(1024)))\n",
        "\n",
        "    for i in nounits:\n",
        "        model.add(keras.layers.Dense(units=i,activation=tact))\n",
        "\n",
        "    model.add(keras.layers.Dense(units=29,activation='sigmoid'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy','mse','mae'])\n",
        "\n",
        "    model.fit(X_train,Y_train,epochs=20,batch_size=200)\n",
        "\n",
        "    y_pred=model.predict(X_test)\n",
        "    pred = list()\n",
        "    for i in range(len(y_pred)):\n",
        "        pred.append(np.argmax(y_pred[i]))\n",
        "    test = list()\n",
        "    for i in range(len(Y_test)):\n",
        "        test.append(np.argmax(Y_test[i]))\n",
        "    acc=accuracy_score(test,pred)\n",
        "    if acc>bestacc:\n",
        "      bestacc=acc\n",
        "      bestunits=nounits.copy()\n",
        "      bestact=tact\n",
        "\n",
        "model=keras.models.Sequential()\n",
        "\n",
        "model.add(keras.layers.Flatten(input_dim=(1024)))\n",
        "\n",
        "#print(bestact)\n",
        "\n",
        "for i in bestunits:\n",
        "    model.add(keras.layers.Dense(units=i,activation=bestact))\n",
        "\n",
        "model.add(keras.layers.Dense(units=29,activation='sigmoid'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy','mse','mae'])\n",
        "\n",
        "model.fit(X_train,Y_train,epochs=80,batch_size=200)\n",
        "\n",
        "y_pred=model.predict(X_test)\n",
        "pred = list()\n",
        "for i in range(len(y_pred)):\n",
        "    pred.append(np.argmax(y_pred[i]))\n",
        "test = list()\n",
        "for i in range(len(Y_test)):\n",
        "    test.append(np.argmax(Y_test[i]))\n",
        "acc=accuracy_score(test,pred)\n",
        "\n",
        "print(acc,bestact,bestunits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "76NWylNrASBT",
        "outputId": "8505b6aa-870b-4216-97e1-799932d5c8bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How many layers do you want the network to have (min 2)? 5\n",
            "Which hyperparameter optimization method to use? (1 for seperate, 2 for random) 1\n",
            "Epoch 1/20\n",
            "188/188 [==============================] - 2s 7ms/step - loss: 3.0862 - accuracy: 0.1340 - mse: 0.2610 - mae: 0.4847\n",
            "Epoch 2/20\n",
            "188/188 [==============================] - 1s 7ms/step - loss: 2.5629 - accuracy: 0.2464 - mse: 0.2636 - mae: 0.4405\n",
            "Epoch 3/20\n",
            "188/188 [==============================] - 1s 7ms/step - loss: 2.2867 - accuracy: 0.3132 - mse: 0.2628 - mae: 0.4146\n",
            "Epoch 4/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 2.1143 - accuracy: 0.3577 - mse: 0.2568 - mae: 0.3942\n",
            "Epoch 5/20\n",
            "188/188 [==============================] - 1s 7ms/step - loss: 1.9895 - accuracy: 0.3897 - mse: 0.2542 - mae: 0.3816\n",
            "Epoch 6/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 1.8884 - accuracy: 0.4154 - mse: 0.2519 - mae: 0.3725\n",
            "Epoch 7/20\n",
            "188/188 [==============================] - 1s 7ms/step - loss: 1.8090 - accuracy: 0.4387 - mse: 0.2488 - mae: 0.3641\n",
            "Epoch 8/20\n",
            "188/188 [==============================] - 1s 7ms/step - loss: 1.7414 - accuracy: 0.4566 - mse: 0.2477 - mae: 0.3585\n",
            "Epoch 9/20\n",
            "188/188 [==============================] - 1s 7ms/step - loss: 1.6791 - accuracy: 0.4755 - mse: 0.2446 - mae: 0.3517\n",
            "Epoch 10/20\n",
            "188/188 [==============================] - 1s 7ms/step - loss: 1.6276 - accuracy: 0.4911 - mse: 0.2437 - mae: 0.3473\n",
            "Epoch 11/20\n",
            "188/188 [==============================] - 1s 7ms/step - loss: 1.5818 - accuracy: 0.5050 - mse: 0.2418 - mae: 0.3425\n",
            "Epoch 12/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 1.5386 - accuracy: 0.5177 - mse: 0.2390 - mae: 0.3372\n",
            "Epoch 13/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 1.5047 - accuracy: 0.5309 - mse: 0.2372 - mae: 0.3331\n",
            "Epoch 14/20\n",
            "188/188 [==============================] - 1s 7ms/step - loss: 1.4660 - accuracy: 0.5392 - mse: 0.2357 - mae: 0.3296\n",
            "Epoch 15/20\n",
            "188/188 [==============================] - 1s 7ms/step - loss: 1.4371 - accuracy: 0.5462 - mse: 0.2347 - mae: 0.3266\n",
            "Epoch 16/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 1.4083 - accuracy: 0.5577 - mse: 0.2325 - mae: 0.3227\n",
            "Epoch 17/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 1.3830 - accuracy: 0.5634 - mse: 0.2327 - mae: 0.3213\n",
            "Epoch 18/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 1.3604 - accuracy: 0.5692 - mse: 0.2328 - mae: 0.3200\n",
            "Epoch 19/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 1.3337 - accuracy: 0.5777 - mse: 0.2309 - mae: 0.3164\n",
            "Epoch 20/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 1.3115 - accuracy: 0.5845 - mse: 0.2310 - mae: 0.3152\n",
            "Epoch 1/20\n",
            "188/188 [==============================] - 2s 6ms/step - loss: 3.3002 - accuracy: 0.0686 - mse: 0.2910 - mae: 0.5333\n",
            "Epoch 2/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 3.0876 - accuracy: 0.1255 - mse: 0.2958 - mae: 0.5266\n",
            "Epoch 3/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 2.9165 - accuracy: 0.1714 - mse: 0.2945 - mae: 0.5102\n",
            "Epoch 4/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 2.7796 - accuracy: 0.2003 - mse: 0.2904 - mae: 0.4937\n",
            "Epoch 5/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 2.6541 - accuracy: 0.2265 - mse: 0.2847 - mae: 0.4775\n",
            "Epoch 6/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 2.5287 - accuracy: 0.2546 - mse: 0.2784 - mae: 0.4612\n",
            "Epoch 7/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 2.4145 - accuracy: 0.2752 - mse: 0.2731 - mae: 0.4453\n",
            "Epoch 8/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 2.3227 - accuracy: 0.2940 - mse: 0.2690 - mae: 0.4316\n",
            "Epoch 9/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 2.2465 - accuracy: 0.3091 - mse: 0.2655 - mae: 0.4193\n",
            "Epoch 10/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 2.1824 - accuracy: 0.3244 - mse: 0.2622 - mae: 0.4091\n",
            "Epoch 11/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 2.1277 - accuracy: 0.3389 - mse: 0.2589 - mae: 0.3994\n",
            "Epoch 12/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 2.0809 - accuracy: 0.3497 - mse: 0.2555 - mae: 0.3910\n",
            "Epoch 13/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 2.0391 - accuracy: 0.3602 - mse: 0.2526 - mae: 0.3834\n",
            "Epoch 14/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 2.0016 - accuracy: 0.3722 - mse: 0.2498 - mae: 0.3768\n",
            "Epoch 15/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 1.9680 - accuracy: 0.3800 - mse: 0.2472 - mae: 0.3704\n",
            "Epoch 16/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 1.9357 - accuracy: 0.3879 - mse: 0.2441 - mae: 0.3641\n",
            "Epoch 17/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 1.9081 - accuracy: 0.3948 - mse: 0.2418 - mae: 0.3593\n",
            "Epoch 18/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 1.8820 - accuracy: 0.4018 - mse: 0.2390 - mae: 0.3540\n",
            "Epoch 19/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 1.8591 - accuracy: 0.4094 - mse: 0.2367 - mae: 0.3494\n",
            "Epoch 20/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 1.8356 - accuracy: 0.4156 - mse: 0.2346 - mae: 0.3453\n",
            "Epoch 1/20\n",
            "188/188 [==============================] - 2s 6ms/step - loss: 3.0322 - accuracy: 0.1535 - mse: 0.2673 - mae: 0.4908\n",
            "Epoch 2/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 2.6730 - accuracy: 0.2394 - mse: 0.2909 - mae: 0.4882\n",
            "Epoch 3/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 2.4137 - accuracy: 0.3007 - mse: 0.3066 - mae: 0.4843\n",
            "Epoch 4/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 2.2201 - accuracy: 0.3421 - mse: 0.3204 - mae: 0.4816\n",
            "Epoch 5/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 2.0799 - accuracy: 0.3747 - mse: 0.3297 - mae: 0.4795\n",
            "Epoch 6/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 1.9665 - accuracy: 0.4040 - mse: 0.3363 - mae: 0.4786\n",
            "Epoch 7/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 1.8758 - accuracy: 0.4272 - mse: 0.3410 - mae: 0.4770\n",
            "Epoch 8/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 1.8025 - accuracy: 0.4470 - mse: 0.3452 - mae: 0.4759\n",
            "Epoch 9/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 1.7399 - accuracy: 0.4631 - mse: 0.3486 - mae: 0.4751\n",
            "Epoch 10/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 1.6850 - accuracy: 0.4777 - mse: 0.3513 - mae: 0.4746\n",
            "Epoch 11/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 1.6319 - accuracy: 0.4936 - mse: 0.3537 - mae: 0.4737\n",
            "Epoch 12/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 1.5907 - accuracy: 0.5080 - mse: 0.3559 - mae: 0.4731\n",
            "Epoch 13/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 1.5534 - accuracy: 0.5171 - mse: 0.3580 - mae: 0.4725\n",
            "Epoch 14/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 1.5153 - accuracy: 0.5278 - mse: 0.3594 - mae: 0.4719\n",
            "Epoch 15/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 1.4870 - accuracy: 0.5354 - mse: 0.3612 - mae: 0.4716\n",
            "Epoch 16/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 1.4580 - accuracy: 0.5473 - mse: 0.3626 - mae: 0.4714\n",
            "Epoch 17/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 1.4312 - accuracy: 0.5525 - mse: 0.3637 - mae: 0.4707\n",
            "Epoch 18/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 1.4021 - accuracy: 0.5615 - mse: 0.3647 - mae: 0.4704\n",
            "Epoch 19/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 1.3796 - accuracy: 0.5683 - mse: 0.3660 - mae: 0.4699\n",
            "Epoch 20/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 1.3561 - accuracy: 0.5765 - mse: 0.3668 - mae: 0.4694\n",
            "Epoch 1/20\n",
            "188/188 [==============================] - 2s 5ms/step - loss: 3.2282 - accuracy: 0.0981 - mse: 0.2523 - mae: 0.4881\n",
            "Epoch 2/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 2.7837 - accuracy: 0.2022 - mse: 0.2395 - mae: 0.4342\n",
            "Epoch 3/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 2.5000 - accuracy: 0.2617 - mse: 0.2216 - mae: 0.3875\n",
            "Epoch 4/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 2.3372 - accuracy: 0.2970 - mse: 0.2168 - mae: 0.3675\n",
            "Epoch 5/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 2.2223 - accuracy: 0.3261 - mse: 0.2155 - mae: 0.3571\n",
            "Epoch 6/20\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 2.1305 - accuracy: 0.3538 - mse: 0.2111 - mae: 0.3455\n",
            "Epoch 7/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 2.0585 - accuracy: 0.3733 - mse: 0.2099 - mae: 0.3384\n",
            "Epoch 8/20\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 2.0011 - accuracy: 0.3863 - mse: 0.2084 - mae: 0.3322\n",
            "Epoch 9/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 1.9487 - accuracy: 0.4007 - mse: 0.2066 - mae: 0.3262\n",
            "Epoch 10/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 1.9077 - accuracy: 0.4114 - mse: 0.2054 - mae: 0.3219\n",
            "Epoch 11/20\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 1.8705 - accuracy: 0.4219 - mse: 0.2071 - mae: 0.3210\n",
            "Epoch 12/20\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 1.8411 - accuracy: 0.4292 - mse: 0.2078 - mae: 0.3194\n",
            "Epoch 13/20\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 1.8106 - accuracy: 0.4366 - mse: 0.2065 - mae: 0.3158\n",
            "Epoch 14/20\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 1.7845 - accuracy: 0.4435 - mse: 0.2069 - mae: 0.3148\n",
            "Epoch 15/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 1.7616 - accuracy: 0.4509 - mse: 0.2063 - mae: 0.3122\n",
            "Epoch 16/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 1.7392 - accuracy: 0.4586 - mse: 0.2054 - mae: 0.3096\n",
            "Epoch 17/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 1.7205 - accuracy: 0.4640 - mse: 0.2066 - mae: 0.3101\n",
            "Epoch 18/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 1.7058 - accuracy: 0.4688 - mse: 0.2065 - mae: 0.3084\n",
            "Epoch 19/20\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 1.6841 - accuracy: 0.4750 - mse: 0.2048 - mae: 0.3054\n",
            "Epoch 20/20\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 1.6716 - accuracy: 0.4754 - mse: 0.2064 - mae: 0.3060\n",
            "Epoch 1/20\n",
            "188/188 [==============================] - 4s 18ms/step - loss: 2.7746 - accuracy: 0.2111 - mse: 0.2481 - mae: 0.4436\n",
            "Epoch 2/20\n",
            "188/188 [==============================] - 3s 18ms/step - loss: 2.0303 - accuracy: 0.3867 - mse: 0.2495 - mae: 0.3913\n",
            "Epoch 3/20\n",
            "188/188 [==============================] - 3s 18ms/step - loss: 1.6339 - accuracy: 0.4959 - mse: 0.2430 - mae: 0.3618\n",
            "Epoch 4/20\n",
            "188/188 [==============================] - 3s 18ms/step - loss: 1.3615 - accuracy: 0.5745 - mse: 0.2403 - mae: 0.3452\n",
            "Epoch 5/20\n",
            "188/188 [==============================] - 3s 18ms/step - loss: 1.1611 - accuracy: 0.6368 - mse: 0.2392 - mae: 0.3342\n",
            "Epoch 6/20\n",
            "188/188 [==============================] - 3s 18ms/step - loss: 0.9901 - accuracy: 0.6863 - mse: 0.2361 - mae: 0.3228\n",
            "Epoch 7/20\n",
            "188/188 [==============================] - 3s 17ms/step - loss: 0.8453 - accuracy: 0.7316 - mse: 0.2321 - mae: 0.3119\n",
            "Epoch 8/20\n",
            "188/188 [==============================] - 3s 17ms/step - loss: 0.7294 - accuracy: 0.7669 - mse: 0.2259 - mae: 0.2996\n",
            "Epoch 9/20\n",
            "188/188 [==============================] - 3s 17ms/step - loss: 0.6189 - accuracy: 0.7968 - mse: 0.2211 - mae: 0.2888\n",
            "Epoch 10/20\n",
            "188/188 [==============================] - 3s 18ms/step - loss: 0.5571 - accuracy: 0.8204 - mse: 0.2181 - mae: 0.2814\n",
            "Epoch 11/20\n",
            "188/188 [==============================] - 3s 18ms/step - loss: 0.4737 - accuracy: 0.8433 - mse: 0.2129 - mae: 0.2717\n",
            "Epoch 12/20\n",
            "188/188 [==============================] - 3s 18ms/step - loss: 0.4203 - accuracy: 0.8590 - mse: 0.2094 - mae: 0.2648\n",
            "Epoch 13/20\n",
            "188/188 [==============================] - 3s 18ms/step - loss: 0.3789 - accuracy: 0.8624 - mse: 0.2087 - mae: 0.2618\n",
            "Epoch 14/20\n",
            "188/188 [==============================] - 3s 18ms/step - loss: 0.3174 - accuracy: 0.8813 - mse: 0.2035 - mae: 0.2525\n",
            "Epoch 15/20\n",
            "188/188 [==============================] - 3s 17ms/step - loss: 0.2866 - accuracy: 0.8867 - mse: 0.2022 - mae: 0.2490\n",
            "Epoch 16/20\n",
            "188/188 [==============================] - 3s 17ms/step - loss: 0.2544 - accuracy: 0.8915 - mse: 0.2007 - mae: 0.2449\n",
            "Epoch 17/20\n",
            "188/188 [==============================] - 3s 18ms/step - loss: 0.2593 - accuracy: 0.8839 - mse: 0.1996 - mae: 0.2420\n",
            "Epoch 18/20\n",
            "188/188 [==============================] - 3s 18ms/step - loss: 0.2398 - accuracy: 0.8854 - mse: 0.1981 - mae: 0.2390\n",
            "Epoch 19/20\n",
            "188/188 [==============================] - 4s 19ms/step - loss: 0.2053 - accuracy: 0.8923 - mse: 0.1958 - mae: 0.2352\n",
            "Epoch 20/20\n",
            "188/188 [==============================] - 4s 19ms/step - loss: 0.1775 - accuracy: 0.8971 - mse: 0.1932 - mae: 0.2309\n",
            "Epoch 1/80\n",
            "188/188 [==============================] - 5s 25ms/step - loss: 2.7349 - accuracy: 0.2037 - mse: 0.2807 - mae: 0.4716\n",
            "Epoch 2/80\n",
            "188/188 [==============================] - 5s 24ms/step - loss: 1.9936 - accuracy: 0.3798 - mse: 0.2841 - mae: 0.4212\n",
            "Epoch 3/80\n",
            "188/188 [==============================] - 5s 25ms/step - loss: 1.6308 - accuracy: 0.4854 - mse: 0.2706 - mae: 0.3866\n",
            "Epoch 4/80\n",
            "188/188 [==============================] - 5s 24ms/step - loss: 1.3826 - accuracy: 0.5558 - mse: 0.2576 - mae: 0.3611\n",
            "Epoch 5/80\n",
            "188/188 [==============================] - 4s 24ms/step - loss: 1.2051 - accuracy: 0.6178 - mse: 0.2464 - mae: 0.3414\n",
            "Epoch 6/80\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 1.0340 - accuracy: 0.6640 - mse: 0.2404 - mae: 0.3278\n",
            "Epoch 7/80\n",
            "188/188 [==============================] - 4s 24ms/step - loss: 0.8860 - accuracy: 0.7094 - mse: 0.2371 - mae: 0.3172\n",
            "Epoch 8/80\n",
            "188/188 [==============================] - 5s 24ms/step - loss: 0.7773 - accuracy: 0.7416 - mse: 0.2316 - mae: 0.3062\n",
            "Epoch 9/80\n",
            "188/188 [==============================] - 5s 24ms/step - loss: 0.6778 - accuracy: 0.7695 - mse: 0.2282 - mae: 0.2977\n",
            "Epoch 10/80\n",
            "188/188 [==============================] - 4s 24ms/step - loss: 0.5951 - accuracy: 0.7947 - mse: 0.2226 - mae: 0.2868\n",
            "Epoch 11/80\n",
            "188/188 [==============================] - 4s 24ms/step - loss: 0.5439 - accuracy: 0.8093 - mse: 0.2212 - mae: 0.2830\n",
            "Epoch 12/80\n",
            "188/188 [==============================] - 5s 24ms/step - loss: 0.4824 - accuracy: 0.8268 - mse: 0.2181 - mae: 0.2765\n",
            "Epoch 13/80\n",
            "188/188 [==============================] - 5s 25ms/step - loss: 0.4208 - accuracy: 0.8449 - mse: 0.2166 - mae: 0.2724\n",
            "Epoch 14/80\n",
            "188/188 [==============================] - 4s 24ms/step - loss: 0.3744 - accuracy: 0.8569 - mse: 0.2120 - mae: 0.2646\n",
            "Epoch 15/80\n",
            "188/188 [==============================] - 4s 24ms/step - loss: 0.3348 - accuracy: 0.8666 - mse: 0.2082 - mae: 0.2586\n",
            "Epoch 16/80\n",
            "188/188 [==============================] - 4s 24ms/step - loss: 0.3008 - accuracy: 0.8721 - mse: 0.2084 - mae: 0.2568\n",
            "Epoch 17/80\n",
            "188/188 [==============================] - 4s 24ms/step - loss: 0.2714 - accuracy: 0.8751 - mse: 0.2070 - mae: 0.2530\n",
            "Epoch 18/80\n",
            "188/188 [==============================] - 5s 24ms/step - loss: 0.2696 - accuracy: 0.8785 - mse: 0.2038 - mae: 0.2495\n",
            "Epoch 19/80\n",
            "188/188 [==============================] - 4s 24ms/step - loss: 0.2608 - accuracy: 0.8773 - mse: 0.2044 - mae: 0.2485\n",
            "Epoch 20/80\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.2445 - accuracy: 0.8793 - mse: 0.2041 - mae: 0.2472\n",
            "Epoch 21/80\n",
            "188/188 [==============================] - 4s 24ms/step - loss: 0.2310 - accuracy: 0.8919 - mse: 0.2012 - mae: 0.2449\n",
            "Epoch 22/80\n",
            "188/188 [==============================] - 5s 24ms/step - loss: 0.2020 - accuracy: 0.8869 - mse: 0.2031 - mae: 0.2442\n",
            "Epoch 23/80\n",
            "188/188 [==============================] - 5s 24ms/step - loss: 0.2114 - accuracy: 0.8841 - mse: 0.2004 - mae: 0.2419\n",
            "Epoch 24/80\n",
            "188/188 [==============================] - 4s 24ms/step - loss: 0.1742 - accuracy: 0.8950 - mse: 0.1954 - mae: 0.2356\n",
            "Epoch 25/80\n",
            "188/188 [==============================] - 5s 24ms/step - loss: 0.1816 - accuracy: 0.8895 - mse: 0.1962 - mae: 0.2350\n",
            "Epoch 26/80\n",
            "188/188 [==============================] - 4s 24ms/step - loss: 0.1772 - accuracy: 0.8881 - mse: 0.1957 - mae: 0.2343\n",
            "Epoch 27/80\n",
            "188/188 [==============================] - 4s 24ms/step - loss: 0.1678 - accuracy: 0.8850 - mse: 0.1946 - mae: 0.2327\n",
            "Epoch 28/80\n",
            "188/188 [==============================] - 4s 24ms/step - loss: 0.1564 - accuracy: 0.8854 - mse: 0.1936 - mae: 0.2305\n",
            "Epoch 29/80\n",
            "188/188 [==============================] - 5s 24ms/step - loss: 0.1651 - accuracy: 0.8839 - mse: 0.1939 - mae: 0.2309\n",
            "Epoch 30/80\n",
            "188/188 [==============================] - 5s 24ms/step - loss: 0.1816 - accuracy: 0.8840 - mse: 0.1953 - mae: 0.2337\n",
            "Epoch 31/80\n",
            "188/188 [==============================] - 4s 24ms/step - loss: 0.1551 - accuracy: 0.8911 - mse: 0.1947 - mae: 0.2317\n",
            "Epoch 32/80\n",
            "188/188 [==============================] - 4s 24ms/step - loss: 0.1509 - accuracy: 0.8890 - mse: 0.1949 - mae: 0.2317\n",
            "Epoch 33/80\n",
            "188/188 [==============================] - 4s 24ms/step - loss: 0.1316 - accuracy: 0.8850 - mse: 0.1971 - mae: 0.2325\n",
            "Epoch 34/80\n",
            "188/188 [==============================] - 5s 24ms/step - loss: 0.1524 - accuracy: 0.8846 - mse: 0.1971 - mae: 0.2336\n",
            "Epoch 35/80\n",
            "188/188 [==============================] - 5s 24ms/step - loss: 0.1411 - accuracy: 0.8898 - mse: 0.1965 - mae: 0.2334\n",
            "Epoch 36/80\n",
            " 96/188 [==============>...............] - ETA: 2s - loss: 0.1086 - accuracy: 0.8886 - mse: 0.1953 - mae: 0.2312"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-4ebcfe4843ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mae'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m             with tf.profiler.experimental.Trace(\n\u001b[1;32m   1378\u001b[0m                 \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36msteps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1244\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1246\u001b[0;31m       \u001b[0moriginal_spe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1247\u001b[0m       can_run_full_execution = (\n\u001b[1;32m   1248\u001b[0m           \u001b[0moriginal_spe\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m     raise NotImplementedError(\n\u001b[1;32m    676\u001b[0m         \"numpy() is only available when eager execution is enabled.\")\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mread_value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0;31m# Return an identity so it can get placed on whatever device the context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0;31m# specifies instead of the device where the variable is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msparse_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[0;34m(input, name)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;31m# variables. Variables have correct handle data when graph building.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m   \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m   \u001b[0;31m# Propagate handle data for happier shape inference for resource variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_handle_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[0;34m(input, name)\u001b[0m\n\u001b[1;32m   4063\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4064\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m-> 4065\u001b[0;31m         _ctx, \"Identity\", name, input)\n\u001b[0m\u001b[1;32m   4066\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4067\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testX=pd.read_csv('X_test.csv')\n",
        "testy=pd.read_csv('y_test.csv')\n",
        "\n",
        "testX = sc.transform(testX)\n",
        "\n",
        "testy=np.asarray(testy)\n",
        "testy=testy.reshape(len(testy),1)\n",
        "testy = ohe.transform(testy)\n",
        "\n",
        "y_pred = model.predict(testX)\n",
        "#Converting predictions to label\n",
        "pred = list()\n",
        "for i in range(len(y_pred)):\n",
        "    pred.append(np.argmax(y_pred[i]))\n",
        "#Converting one hot encoded test label to label\n",
        "test = list()\n",
        "for i in range(len(testy)):\n",
        "    test.append(np.argmax(testy[i]))\n",
        "print(pred)\n",
        "print(test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyDGJ03WFFPg",
        "outputId": "b2523d81-33c3-4533-d87c-d6ad3dd9a39e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8, 0, 0, 0, 0, 0, 0, 28, 0, 0, 0, 0, 0, 0, 0, 0, 0, 28, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 27, 0, 0, 0, 0, 24, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 24, 0, 0, 0, 0, 0, 28, 0, 0, 0, 25, 0, 0, 0, 22, 0, 17, 8, 0, 3, 17, 0, 0, 0, 0, 0, 0, 27, 0, 28, 0, 0, 0, 0, 0, 0, 19, 0, 0, 4, 0, 0, 0, 0, 0, 3, 0, 0, 17, 0, 27, 0, 0, 0, 8, 0, 0, 28, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 28, 8, 0, 0, 0, 0, 0, 0, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 28, 0, 0, 0, 0, 0, 18, 22, 0, 0, 0, 8, 28, 0, 0, 0, 0, 0, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 28, 0, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 28, 0, 0, 28, 0, 0, 0, 0, 0, 0, 23, 22, 0, 0, 0, 0, 22, 0, 0, 0, 0, 0, 0, 6, 0, 0, 28, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 28, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 22, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 28, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 10, 28, 0, 0, 28, 0, 0, 0, 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 25, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 18, 0, 24, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 22, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 28, 0, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 23, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 28, 0, 0, 0, 0, 28, 0, 17, 0, 0, 0, 0, 0, 0, 0, 1, 0, 18, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 23, 23, 13, 1, 1, 1, 1, 1, 27, 24, 1, 3, 1, 1, 27, 1, 1, 1, 1, 27, 4, 0, 1, 1, 22, 1, 1, 1, 1, 4, 1, 1, 1, 22, 19, 14, 1, 1, 1, 1, 17, 1, 27, 1, 1, 2, 27, 1, 1, 11, 1, 1, 1, 1, 1, 27, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 11, 1, 1, 1, 1, 27, 1, 28, 1, 1, 1, 4, 1, 1, 7, 1, 1, 1, 1, 1, 1, 27, 1, 27, 1, 1, 1, 1, 1, 1, 27, 1, 2, 1, 1, 1, 11, 1, 15, 24, 1, 1, 1, 1, 1, 1, 22, 23, 1, 1, 25, 1, 1, 1, 1, 27, 1, 1, 1, 1, 1, 1, 1, 14, 11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 27, 1, 27, 1, 1, 11, 1, 22, 25, 1, 27, 11, 1, 4, 1, 24, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 28, 1, 1, 1, 1, 1, 1, 1, 1, 12, 0, 4, 1, 1, 24, 1, 1, 3, 1, 27, 1, 1, 1, 1, 24, 1, 1, 1, 1, 4, 22, 23, 27, 1, 27, 11, 1, 1, 28, 1, 1, 1, 19, 1, 27, 1, 23, 1, 1, 24, 1, 1, 1, 1, 1, 1, 1, 1, 1, 27, 1, 1, 27, 1, 27, 27, 1, 1, 1, 1, 1, 1, 1, 22, 1, 1, 25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 27, 1, 1, 1, 17, 1, 3, 28, 1, 1, 1, 1, 22, 0, 1, 1, 1, 17, 1, 1, 1, 1, 24, 1, 1, 23, 23, 1, 1, 1, 1, 27, 2, 1, 1, 1, 11, 1, 22, 1, 1, 27, 24, 13, 1, 1, 11, 1, 1, 1, 1, 1, 1, 1, 23, 1, 28, 1, 1, 27, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 0, 22, 1, 27, 11, 1, 1, 1, 15, 1, 1, 25, 1, 1, 1, 1, 1, 1, 1, 1, 25, 1, 2, 27, 1, 2, 3, 5, 2, 3, 2, 2, 3, 12, 12, 2, 18, 6, 20, 24, 20, 2, 3, 2, 2, 2, 2, 2, 24, 24, 24, 24, 2, 21, 3, 19, 5, 2, 2, 2, 12, 2, 2, 22, 2, 2, 24, 3, 2, 2, 2, 2, 19, 3, 2, 2, 21, 12, 21, 6, 0, 2, 28, 3, 2, 2, 19, 12, 24, 2, 2, 28, 8, 12, 2, 1, 12, 13, 12, 2, 24, 25, 2, 7, 24, 24, 3, 3, 24, 13, 24, 28, 18, 12, 3, 22, 28, 2, 24, 2, 24, 2, 2, 24, 2, 11, 2, 2, 27, 3, 3, 2, 3, 2, 3, 2, 3, 2, 3, 20, 2, 16, 22, 17, 2, 24, 27, 3, 2, 2, 6, 28, 28, 20, 24, 2, 3, 20, 13, 2, 3, 22, 2, 22, 3, 3, 2, 2, 17, 2, 3, 1, 3, 2, 2, 7, 2, 3, 3, 2, 2, 2, 12, 3, 2, 28, 3, 2, 0, 28, 3, 2, 3, 12, 24, 24, 2, 2, 17, 2, 10, 24, 24, 3, 12, 2, 16, 8, 24, 1, 2, 3, 11, 2, 24, 18, 24, 2, 14, 3, 2, 2, 12, 3, 12, 2, 28, 2, 2, 6, 11, 19, 19, 2, 18, 3, 14, 2, 2, 2, 8, 3, 24, 2, 24, 24, 2, 24, 2, 2, 2, 2, 2, 2, 20, 2, 28, 20, 24, 27, 3, 15, 2, 3, 2, 2, 19, 24, 3, 2, 3, 24, 2, 3, 11, 1, 2, 3, 2, 3, 24, 12, 3, 3, 3, 20, 24, 8, 3, 3, 20, 12, 12, 24, 19, 3, 3, 2, 24, 2, 11, 25, 2, 25, 2, 3, 24, 10, 2, 3, 2, 2, 14, 0, 2, 3, 24, 2, 25, 2, 12, 2, 11, 12, 24, 2, 4, 2, 3, 8, 3, 28, 7, 2, 1, 2, 2, 2, 2, 2, 2, 24, 3, 21, 24, 2, 0, 2, 2, 14, 2, 3, 2, 12, 3, 3, 6, 16, 24, 12, 3, 2, 3, 8, 2, 3, 2, 27, 2, 16, 2, 2, 3, 3, 12, 3, 2, 2, 2, 24, 2, 3, 2, 2, 2, 2, 3, 24, 28, 18, 22, 12, 3, 3, 3, 0, 24, 3, 3, 24, 24, 24, 3, 2, 6, 3, 3, 12, 3, 5, 16, 2, 7, 8, 3, 3, 0, 3, 3, 3, 6, 2, 3, 8, 6, 3, 12, 3, 3, 2, 3, 3, 16, 3, 28, 3, 19, 3, 3, 3, 20, 24, 3, 19, 3, 3, 28, 6, 5, 0, 15, 28, 3, 3, 24, 12, 2, 24, 3, 3, 3, 2, 8, 20, 6, 0, 3, 22, 2, 22, 3, 3, 3, 3, 24, 24, 24, 2, 28, 3, 21, 12, 2, 22, 3, 3, 3, 28, 3, 3, 3, 20, 11, 24, 7, 3, 3, 6, 3, 2, 3, 22, 8, 16, 16, 2, 3, 3, 3, 3, 20, 3, 24, 12, 24, 3, 2, 2, 2, 24, 3, 28, 3, 2, 3, 3, 2, 3, 12, 21, 12, 24, 3, 2, 20, 3, 2, 3, 3, 3, 2, 28, 22, 24, 3, 3, 2, 20, 2, 21, 12, 3, 5, 2, 3, 3, 3, 28, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2, 1, 3, 3, 4, 3, 24, 3, 24, 3, 3, 3, 4, 24, 6, 3, 3, 3, 17, 3, 2, 2, 24, 2, 5, 12, 12, 3, 24, 3, 3, 3, 3, 3, 21, 2, 3, 21, 6, 22, 24, 3, 3, 25, 28, 3, 28, 3, 3, 3, 24, 24, 2, 2, 11, 2, 24, 21, 2, 3, 3, 7, 2, 3, 6, 10, 3, 3, 27, 23, 3, 28, 16, 3, 3, 2, 3, 6, 3, 3, 14, 3, 16, 3, 2, 3, 3, 3, 3, 2, 24, 24, 3, 7, 2, 12, 16, 2, 2, 6, 3, 3, 3, 3, 3, 3, 3, 27, 3, 2, 3, 2, 3, 3, 2, 28, 3, 27, 18, 2, 7, 3, 24, 22, 20, 28, 2, 24, 3, 2, 3, 3, 3, 12, 24, 3, 3, 3, 3, 3, 11, 3, 24, 28, 3, 3, 3, 22, 0, 3, 12, 3, 3, 2, 22, 21, 3, 3, 3, 2, 2, 18, 3, 3, 18, 12, 3, 6, 3, 3, 28, 3, 3, 3, 22, 2, 6, 20, 24, 3, 3, 3, 3, 3, 12, 4, 4, 17, 4, 4, 4, 27, 25, 4, 20, 4, 25, 4, 23, 4, 25, 26, 28, 14, 4, 4, 18, 4, 5, 5, 17, 4, 17, 17, 0, 4, 17, 14, 17, 25, 5, 4, 28, 2, 4, 27, 5, 13, 25, 4, 4, 5, 4, 1, 4, 4, 5, 27, 4, 17, 4, 4, 18, 4, 4, 3, 4, 4, 4, 4, 4, 4, 1, 23, 4, 4, 4, 4, 4, 20, 4, 18, 4, 5, 2, 5, 2, 5, 17, 4, 25, 4, 4, 4, 4, 4, 5, 4, 27, 25, 6, 4, 25, 2, 4, 4, 17, 20, 5, 2, 18, 28, 26, 4, 25, 28, 4, 14, 17, 23, 27, 4, 15, 4, 4, 26, 4, 4, 4, 27, 27, 1, 4, 4, 4, 25, 25, 23, 23, 6, 4, 26, 25, 4, 4, 17, 4, 25, 17, 17, 27, 5, 6, 13, 17, 4, 25, 17, 4, 28, 18, 6, 5, 4, 4, 4, 4, 5, 23, 4, 1, 4, 4, 4, 25, 4, 21, 4, 27, 1, 4, 4, 27, 4, 16, 23, 23, 6, 17, 4, 4, 17, 4, 4, 27, 12, 4, 4, 4, 4, 16, 4, 4, 4, 17, 4, 18, 4, 5, 4, 1, 4, 4, 6, 5, 23, 4, 17, 4, 1, 4, 21, 6, 22, 5, 16, 4, 1, 1, 4, 4, 13, 17, 4, 28, 6, 5, 4, 4, 5, 28, 23, 1, 6, 4, 6, 4, 4, 4, 27, 5, 7, 4, 4, 4, 1, 27, 4, 1, 19, 4, 4, 4, 4, 17, 11, 5, 5, 23, 5, 11, 1, 4, 5, 27, 4, 26, 4, 5, 4, 27, 4, 23, 4, 5, 4, 4, 23, 5, 5, 4, 17, 5, 23, 4, 4, 17, 4, 4, 4, 27, 17, 23, 1, 6, 4, 4, 4, 5, 4, 4, 4, 5, 4, 4, 27, 5, 4, 4, 4, 4, 12, 4, 4, 17, 17, 4, 4, 27, 28, 4, 4, 12, 28, 4, 4, 23, 17, 4, 4, 12, 4, 4, 4, 4, 4, 26, 17, 5, 13, 17, 14, 4, 17, 4, 4, 23, 27, 4, 4, 4, 6, 25, 6, 15, 4, 4, 6, 4, 4, 4, 4, 4, 23, 1, 3, 4, 4, 5, 5, 5, 5, 6, 5, 17, 5, 5, 5, 6, 5, 13, 5, 5, 4, 4, 4, 4, 22, 5, 5, 4, 6, 5, 28, 4, 4, 6, 5, 5, 4, 4, 5, 23, 28, 22, 28, 5, 5, 17, 4, 11, 5, 5, 5, 19, 4, 11, 26, 17, 6, 25, 11, 5, 5, 17, 4, 17, 5, 4, 5, 17, 17, 4, 4, 7, 4, 17, 25, 4, 4, 13, 5, 5, 5, 5, 27, 26, 9, 7, 5, 6, 6, 5, 6, 4, 5, 7, 6, 4, 5, 4, 5, 17, 6, 4, 13, 5, 5, 23, 4, 4, 5, 6, 21, 4, 4, 21, 5, 5, 27, 4, 5, 25, 5, 17, 7, 4, 28, 11, 3, 5, 5, 5, 11, 5, 18, 17, 5, 4, 5, 5, 20, 4, 4, 17, 17, 7, 13, 21, 2, 17, 14, 17, 28, 6, 28, 6, 19, 13, 5, 5, 3, 23, 21, 4, 9, 4, 5, 5, 17, 4, 4, 5, 23, 4, 4, 0, 4, 13, 4, 4, 5, 13, 14, 5, 9, 28, 17, 4, 5, 7, 6, 23, 5, 20, 5, 19, 4, 4, 6, 5, 17, 5, 26, 5, 5, 17, 5, 4, 4, 4, 5, 5, 5, 5, 21, 5, 5, 12, 5, 27, 7, 4, 4, 4, 21, 5, 5, 25, 5, 0, 23, 5, 4, 21, 19, 5, 4, 4, 14, 5, 17, 6, 5, 4, 4, 11, 4, 28, 5, 5, 7, 4, 5, 6, 4, 19, 4, 5, 5, 11, 6, 6, 4, 5, 5, 5, 5, 23, 4, 7, 6, 4, 8, 5, 5, 5, 5, 1, 4, 5, 5, 4, 16, 5, 5, 5, 17, 5, 5, 4, 5, 5, 5, 17, 4, 7, 5, 23, 12, 6, 4, 4, 4, 5, 5, 4, 20, 18, 4, 4, 4, 21, 5, 5, 15, 5, 8, 2, 6, 21, 5, 5, 5, 5, 5, 23, 5, 28, 17, 15, 5, 14, 4, 17, 4, 4, 5, 4, 17, 11, 5, 5, 5, 13, 5, 7, 4, 26, 4, 6, 4, 14, 27, 4, 4, 2, 11, 17, 5, 4, 3, 5, 14, 5, 15, 4, 25, 7, 5, 5, 5, 20, 5, 4, 5, 24, 28, 17, 25, 18, 3, 6, 6, 28, 12, 28, 18, 15, 15, 6, 8, 6, 6, 4, 6, 6, 6, 4, 5, 20, 6, 6, 6, 22, 6, 6, 6, 5, 17, 16, 3, 6, 26, 27, 24, 6, 3, 6, 4, 6, 6, 15, 6, 18, 7, 6, 19, 6, 18, 6, 17, 6, 6, 19, 8, 11, 19, 6, 18, 5, 5, 20, 28, 6, 6, 24, 6, 27, 6, 6, 12, 16, 6, 6, 17, 6, 19, 4, 6, 6, 5, 5, 5, 6, 4, 6, 24, 4, 6, 18, 6, 6, 14, 14, 6, 4, 6, 6, 6, 6, 6, 6, 6, 26, 4, 27, 16, 4, 6, 18, 6, 6, 6, 28, 6, 6, 6, 12, 6, 6, 0, 17, 6, 6, 6, 6, 6, 18, 6, 6, 6, 0, 6, 8, 6, 22, 18, 6, 6, 4, 18, 24, 5, 20, 19, 3, 6, 2, 2, 6, 28, 15, 6, 6, 6, 6, 6, 6, 28, 8, 3, 6, 4, 20, 6, 6, 28, 5, 6, 0, 6, 14, 6, 13, 16, 6, 6, 6, 5, 12, 6, 6, 6, 6, 19, 6, 20, 19, 6, 20, 16, 4, 12, 6, 12, 18, 5, 18, 6, 10, 23, 19, 6, 6, 19, 17, 6, 17, 6, 6, 4, 6, 17, 12, 8, 6, 18, 6, 5, 17, 18, 14, 14, 6, 6, 5, 6, 4, 6, 19, 18, 18, 18, 6, 5, 5, 6, 6, 3, 6, 20, 6, 6, 6, 4, 3, 20, 15, 6, 17, 6, 6, 6, 15, 11, 28, 5, 28, 6, 6, 3, 6, 6, 0, 19, 6, 6, 6, 6, 6, 6, 0, 28, 24, 6, 19, 6, 6, 19, 19, 18, 6, 5, 14, 6, 4, 6, 28, 5, 15, 6, 12, 4, 6, 5, 20, 6, 6, 4, 6, 5, 28, 4, 17, 4, 5, 6, 6, 5, 15, 19, 5, 6, 6, 6, 18, 18, 6, 14, 6, 18, 12, 6, 15, 6, 6, 4, 20, 10, 26, 6, 6, 6, 6, 15, 5, 6, 15, 19, 6, 5, 6, 16, 4, 3, 4, 6, 8, 3, 3, 5, 6, 28, 4, 9, 5, 18, 0, 5, 14, 0, 6, 0, 6, 6, 1, 15, 7, 1, 5, 15, 7, 7, 8, 7, 8, 3, 7, 21, 23, 3, 7, 7, 5, 24, 16, 28, 7, 7, 22, 7, 7, 7, 25, 7, 7, 20, 5, 7, 7, 5, 7, 5, 7, 7, 19, 20, 22, 20, 7, 20, 4, 7, 7, 8, 7, 28, 7, 19, 6, 3, 2, 7, 11, 7, 7, 7, 7, 9, 2, 26, 6, 6, 18, 5, 7, 3, 2, 7, 25, 7, 7, 7, 24, 7, 15, 28, 8, 26, 7, 7, 8, 15, 6, 7, 5, 7, 7, 22, 11, 5, 28, 5, 7, 28, 7, 7, 7, 26, 8, 5, 5, 7, 8, 8, 20, 11, 28, 7, 28, 15, 5, 1, 8, 24, 28, 7, 9, 7, 9, 8, 7, 7, 15, 15, 2, 20, 28, 22, 19, 7, 7, 17, 7, 7, 7, 26, 15, 15, 7, 7, 18, 7, 7, 25, 22, 22, 26, 12, 8, 0, 4, 15, 22, 7, 19, 25, 3, 7, 7, 7, 15, 7, 8, 12, 28, 4, 2, 7, 2, 7, 8, 6, 8, 8, 3, 8, 24, 28, 28, 24, 6, 15, 12, 6, 8, 6, 8, 12, 6, 18, 8, 15, 6, 6, 8, 8, 3, 24, 20, 8, 3, 8, 24, 8, 8, 8, 8, 6, 14, 8, 8, 8, 8, 24, 8, 8, 8, 24, 8, 6, 8, 8, 24, 15, 8, 10, 8, 6, 28, 8, 7, 8, 6, 8, 6, 8, 24, 8, 22, 8, 3, 0, 8, 0, 28, 8, 6, 24, 28, 0, 22, 24, 8, 28, 28, 8, 8, 8, 3, 22, 8, 8, 8, 8, 18, 16, 8, 8, 6, 7, 8, 8, 22, 4, 28, 24, 8, 8, 8, 8, 22, 8, 15, 6, 8, 8, 21, 24, 8, 8, 24, 10, 8, 8, 6, 8, 8, 18, 3, 8, 6, 8, 8, 8, 22, 10, 8, 8, 8, 24, 0, 16, 8, 8, 24, 8, 8, 15, 3, 3, 8, 8, 8, 28, 7, 8, 8, 6, 20, 22, 8, 8, 7, 8, 8, 8, 3, 10, 3, 7, 8, 8, 6, 7, 0, 28, 9, 9, 9, 9, 9, 22, 9, 9, 11, 9, 26, 9, 9, 1, 22, 9, 28, 18, 9, 9, 9, 9, 9, 17, 11, 28, 9, 9, 10, 9, 22, 19, 26, 9, 17, 9, 9, 26, 9, 0, 9, 23, 9, 9, 17, 9, 9, 9, 9, 23, 9, 9, 9, 9, 17, 9, 22, 27, 9, 11, 9, 9, 4, 21, 3, 26, 9, 24, 9, 23, 1, 9, 9, 22, 9, 9, 9, 9, 9, 9, 22, 26, 10, 9, 26, 14, 9, 9, 10, 7, 9, 28, 28, 22, 9, 9, 9, 9, 23, 9, 9, 10, 10, 9, 7, 9, 9, 4, 0, 9, 7, 11, 28, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 9, 9, 9, 10, 9, 9, 22, 5, 5, 10, 9, 9, 9, 9, 22, 9, 27, 9, 9, 9, 9, 9, 9, 11, 10, 5, 9, 9, 9, 9, 7, 9, 9, 13, 9, 9, 27, 10, 12, 9, 9, 23, 10, 9, 3, 0, 10, 3, 10, 6, 8, 14, 8, 10, 28, 9, 9, 22, 10, 22, 10, 10, 24, 9, 10, 10, 10, 14, 10, 9, 10, 10, 9, 9, 9, 3, 0, 10, 0, 0, 9, 10, 8, 10, 22, 10, 0, 8, 24, 10, 10, 8, 10, 28, 10, 8, 8, 0, 6, 27, 10, 10, 10, 10, 22, 14, 10, 9, 0, 22, 8, 10, 9, 10, 8, 10, 0, 22, 19, 8, 10, 9, 8, 26, 10, 10, 10, 18, 9, 10, 9, 24, 9, 27, 22, 10, 10, 10, 10, 18, 10, 8, 9, 10, 10, 8, 10, 10, 8, 10, 10, 9, 22, 9, 8, 9, 9, 10, 8, 10, 0, 3, 10, 24, 10, 28, 8, 22, 6, 8, 28, 8, 10, 9, 9, 10, 9, 24, 10, 10, 9, 10, 19, 10, 10, 9, 8, 10, 6, 12, 6, 7, 15, 14, 10, 9, 10, 10, 10, 10, 8, 12, 14, 10, 10, 8, 0, 10, 10, 10, 10, 9, 22, 10, 10, 22, 7, 11, 11, 11, 13, 11, 13, 23, 11, 13, 14, 11, 11, 11, 11, 13, 11, 11, 11, 4, 11, 11, 11, 26, 16, 11, 27, 13, 24, 26, 24, 11, 5, 13, 11, 13, 11, 24, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 7, 11, 11, 13, 11, 11, 11, 11, 1, 27, 5, 11, 24, 13, 23, 9, 21, 11, 11, 11, 11, 24, 13, 11, 5, 5, 11, 2, 11, 11, 17, 24, 11, 25, 13, 14, 11, 11, 11, 11, 13, 11, 11, 11, 13, 11, 11, 11, 14, 11, 11, 13, 11, 11, 11, 11, 11, 11, 24, 13, 24, 11, 11, 11, 14, 24, 14, 11, 11, 13, 2, 2, 1, 11, 14, 11, 11, 11, 11, 11, 11, 11, 2, 13, 11, 11, 11, 11, 11, 11, 11, 5, 11, 3, 11, 11, 1, 11, 11, 14, 11, 11, 13, 11, 11, 4, 4, 17, 0, 11, 11, 11, 1, 11, 11, 11, 11, 11, 13, 11, 13, 11, 1, 22, 4, 13, 11, 22, 11, 11, 14, 12, 11, 1, 13, 11, 1, 11, 11, 13, 11, 11, 11, 26, 11, 11, 27, 11, 13, 13, 17, 11, 11, 24, 11, 1, 11, 11, 21, 11, 1, 13, 11, 15, 6, 27, 11, 27, 11, 14, 11, 11, 2, 11, 11, 13, 13, 11, 11, 11, 11, 13, 11, 13, 11, 12, 11, 9, 11, 11, 14, 11, 5, 2, 11, 11, 27, 11, 11, 11, 13, 11, 11, 11, 14, 11, 11, 11, 11, 11, 11, 13, 11, 11, 11, 11, 11, 14, 11, 11, 13, 13, 13, 23, 13, 11, 11, 7, 11, 23, 11, 11, 11, 23, 13, 11, 14, 13, 11, 27, 11, 13, 14, 11, 11, 11, 12, 11, 3, 11, 5, 11, 11, 11, 13, 11, 11, 1, 11, 11, 14, 23, 11, 1, 11, 24, 11, 14, 11, 11, 11, 11, 11, 11, 3, 11, 11, 11, 13, 11, 11, 11, 5, 1, 11, 5, 14, 27, 24, 11, 11, 11, 11, 11, 15, 11, 14, 11, 12, 13, 3, 6, 14, 12, 19, 12, 12, 12, 12, 3, 19, 11, 12, 12, 11, 12, 12, 6, 10, 12, 12, 20, 12, 12, 28, 12, 16, 12, 3, 14, 12, 17, 14, 12, 3, 12, 4, 6, 3, 12, 3, 13, 12, 20, 12, 10, 12, 24, 28, 1, 19, 3, 21, 12, 6, 12, 12, 12, 12, 24, 4, 12, 19, 20, 14, 19, 12, 12, 24, 12, 14, 14, 12, 12, 24, 12, 3, 12, 15, 2, 19, 6, 12, 14, 17, 12, 16, 27, 20, 18, 12, 19, 2, 12, 12, 12, 2, 18, 3, 6, 12, 17, 12, 6, 20, 12, 11, 12, 12, 14, 3, 12, 12, 24, 15, 12, 20, 12, 3, 22, 12, 3, 22, 12, 12, 16, 12, 21, 12, 12, 2, 3, 12, 12, 22, 12, 11, 25, 20, 12, 2, 5, 12, 20, 12, 12, 12, 14, 12, 12, 12, 3, 14, 12, 12, 12, 12, 24, 6, 12, 12, 12, 11, 2, 12, 12, 16, 6, 24, 12, 11, 3, 6, 24, 12, 17, 12, 24, 20, 28, 12, 20, 12, 12, 12, 3, 19, 3, 2, 6, 19, 21, 12, 12, 14, 12, 11, 3, 12, 6, 24, 20, 13, 12, 3, 12, 19, 12, 12, 20, 12, 2, 12, 12, 22, 20, 12, 12, 12, 12, 12, 28, 3, 12, 10, 6, 12, 21, 3, 12, 12, 23, 27, 12, 10, 3, 12, 12, 12, 11, 16, 24, 14, 20, 12, 16, 12, 28, 14, 24, 24, 2, 2, 2, 12, 28, 12, 6, 21, 12, 12, 12, 12, 28, 12, 19, 10, 6, 21, 12, 12, 3, 3, 20, 21, 12, 12, 12, 14, 12, 12, 20, 16, 12, 12, 12, 24, 12, 24, 12, 6, 17, 12, 12, 15, 3, 14, 12, 12, 12, 12, 12, 12, 14, 19, 12, 14, 19, 12, 12, 12, 12, 24, 12, 20, 3, 12, 14, 16, 14, 16, 24, 2, 24, 24, 24, 6, 2, 12, 3, 21, 3, 12, 24, 12, 25, 3, 3, 12, 3, 13, 13, 13, 13, 11, 13, 16, 23, 13, 13, 14, 23, 14, 11, 11, 3, 23, 13, 12, 13, 13, 13, 11, 11, 23, 11, 11, 14, 11, 13, 25, 11, 13, 13, 13, 13, 25, 13, 13, 11, 19, 11, 13, 13, 14, 9, 14, 19, 15, 17, 1, 17, 14, 23, 13, 13, 13, 11, 13, 13, 13, 11, 15, 13, 11, 13, 14, 11, 14, 11, 13, 25, 11, 15, 11, 13, 13, 13, 14, 13, 11, 11, 13, 11, 11, 28, 11, 14, 11, 11, 11, 27, 13, 13, 13, 28, 14, 11, 11, 13, 13, 23, 23, 13, 13, 11, 6, 26, 23, 11, 11, 14, 11, 1, 4, 11, 13, 13, 11, 11, 13, 11, 13, 11, 13, 13, 13, 14, 13, 11, 11, 1, 11, 11, 13, 13, 11, 14, 13, 11, 13, 11, 13, 11, 11, 13, 11, 13, 20, 24, 5, 13, 13, 13, 11, 11, 13, 13, 13, 23, 25, 11, 13, 11, 4, 13, 14, 13, 25, 11, 14, 11, 13, 11, 13, 11, 11, 23, 11, 6, 13, 23, 14, 14, 13, 13, 25, 23, 11, 14, 11, 13, 23, 13, 13, 13, 11, 11, 13, 17, 13, 23, 15, 16, 13, 13, 23, 13, 15, 13, 13, 11, 13, 18, 23, 13, 27, 12, 11, 25, 15, 13, 11, 13, 13, 11, 13, 25, 23, 17, 13, 13, 14, 14, 13, 13, 12, 13, 11, 13, 11, 13, 13, 11, 14, 11, 11, 11, 12, 11, 11, 14, 13, 13, 13, 13, 11, 13, 22, 13, 23, 13, 13, 13, 13, 17, 23, 11, 13, 13, 13, 13, 13, 13, 11, 13, 11, 25, 4, 13, 11, 14, 14, 4, 13, 11, 11, 21, 13, 13, 11, 13, 14, 11, 11, 14, 14, 13, 13, 13, 13, 13, 17, 13, 13, 13, 11, 13, 13, 13, 23, 13, 13, 11, 4, 14, 17, 11, 11, 25, 11, 27, 13, 11, 11, 11, 11, 11, 13, 11, 11, 13, 14, 13, 13, 14, 14, 12, 13, 13, 17, 13, 11, 13, 17, 3, 14, 12, 12, 20, 14, 12, 14, 14, 12, 11, 14, 16, 19, 14, 13, 17, 14, 14, 16, 13, 14, 14, 12, 21, 16, 20, 3, 14, 14, 13, 14, 13, 18, 14, 23, 11, 14, 1, 14, 13, 15, 14, 13, 13, 14, 14, 23, 13, 12, 11, 16, 14, 14, 11, 14, 11, 28, 19, 14, 13, 14, 14, 20, 5, 17, 14, 14, 21, 14, 13, 14, 25, 6, 14, 14, 6, 23, 16, 13, 4, 25, 12, 24, 17, 14, 12, 12, 14, 12, 6, 14, 19, 20, 14, 12, 19, 10, 12, 14, 14, 14, 14, 18, 14, 14, 12, 12, 14, 14, 14, 14, 12, 13, 13, 20, 20, 14, 12, 19, 14, 11, 6, 14, 12, 14, 14, 14, 14, 13, 14, 11, 24, 14, 14, 14, 14, 13, 14, 11, 14, 18, 22, 14, 14, 16, 20, 14, 13, 13, 12, 14, 13, 14, 16, 14, 11, 21, 17, 14, 25, 14, 14, 14, 1, 14, 14, 14, 20, 8, 19, 14, 14, 25, 14, 14, 10, 14, 15, 14, 14, 14, 25, 12, 14, 14, 20, 13, 3, 25, 14, 12, 14, 12, 14, 15, 19, 14, 14, 14, 14, 19, 15, 19, 14, 25, 14, 14, 14, 14, 14, 13, 13, 19, 14, 14, 14, 14, 13, 13, 21, 11, 18, 6, 13, 19, 20, 11, 12, 13, 14, 14, 12, 14, 22, 7, 20, 18, 13, 12, 14, 14, 12, 14, 14, 14, 14, 13, 14, 12, 12, 11, 14, 13, 18, 14, 14, 13, 12, 12, 14, 14, 10, 12, 14, 19, 19, 19, 14, 14, 11, 14, 11, 19, 14, 16, 25, 14, 20, 14, 19, 15, 14, 14, 12, 14, 14, 19, 14, 14, 14, 12, 19, 14, 11, 14, 6, 14, 14, 15, 11, 14, 14, 11, 16, 12, 14, 19, 19, 11, 12, 14, 0, 25, 14, 14, 14, 14, 14, 14, 14, 14, 20, 14, 22, 11, 11, 12, 14, 14, 19, 25, 14, 19, 14, 14, 12, 13, 16, 16, 15, 3, 16, 15, 15, 15, 15, 23, 27, 15, 25, 15, 4, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 12, 12, 15, 15, 15, 5, 15, 11, 15, 15, 16, 21, 15, 15, 15, 15, 15, 6, 15, 15, 16, 15, 14, 15, 15, 15, 15, 18, 7, 15, 14, 16, 19, 11, 15, 15, 19, 25, 6, 16, 6, 16, 16, 15, 15, 15, 15, 15, 25, 15, 14, 16, 15, 15, 6, 8, 15, 22, 15, 15, 15, 15, 18, 15, 0, 15, 21, 15, 15, 15, 19, 15, 15, 3, 6, 14, 21, 11, 15, 15, 20, 22, 15, 11, 15, 16, 15, 15, 15, 15, 15, 13, 25, 15, 13, 15, 15, 15, 15, 28, 15, 15, 15, 15, 6, 14, 11, 15, 15, 15, 15, 0, 15, 15, 15, 15, 15, 15, 14, 15, 15, 16, 15, 15, 16, 19, 15, 15, 8, 25, 16, 20, 15, 15, 22, 16, 25, 15, 15, 15, 14, 15, 15, 15, 15, 15, 15, 15, 15, 14, 12, 25, 24, 15, 15, 16, 15, 15, 15, 19, 15, 16, 15, 4, 12, 15, 26, 16, 14, 16, 15, 12, 15, 18, 14, 15, 15, 0, 14, 8, 17, 15, 15, 15, 15, 15, 15, 6, 14, 16, 14, 15, 15, 15, 16, 0, 15, 15, 15, 16, 16, 15, 28, 15, 16, 6, 15, 15, 15, 15, 8, 14, 6, 15, 15, 16, 0, 15, 15, 8, 15, 15, 17, 16, 24, 16, 18, 15, 8, 15, 15, 15, 15, 15, 15, 16, 18, 15, 15, 15, 15, 20, 15, 17, 15, 7, 15, 16, 15, 4, 28, 15, 15, 4, 6, 15, 15, 15, 12, 18, 16, 13, 15, 15, 15, 15, 19, 15, 28, 15, 16, 15, 8, 15, 15, 11, 15, 2, 19, 25, 15, 15, 15, 15, 6, 15, 20, 13, 15, 15, 14, 16, 27, 13, 15, 15, 15, 5, 26, 28, 15, 15, 15, 15, 15, 15, 15, 6, 15, 25, 15, 15, 15, 13, 15, 21, 15, 15, 21, 16, 16, 14, 15, 15, 14, 16, 16, 25, 15, 15, 15, 16, 16, 16, 4, 16, 16, 15, 16, 14, 21, 15, 14, 16, 14, 16, 20, 6, 16, 20, 16, 16, 28, 16, 25, 16, 15, 15, 15, 15, 16, 25, 25, 0, 16, 16, 25, 20, 16, 15, 25, 14, 16, 7, 15, 16, 20, 20, 15, 16, 28, 15, 15, 15, 28, 12, 15, 15, 14, 15, 16, 16, 15, 12, 14, 16, 14, 16, 14, 15, 15, 15, 16, 15, 15, 15, 16, 15, 15, 15, 16, 15, 15, 14, 15, 16, 16, 15, 14, 16, 28, 16, 16, 15, 6, 5, 25, 15, 14, 16, 15, 15, 15, 16, 22, 28, 16, 2, 15, 12, 16, 16, 16, 12, 16, 16, 20, 16, 14, 16, 8, 12, 16, 16, 3, 0, 16, 18, 15, 19, 16, 19, 14, 15, 16, 15, 15, 14, 12, 15, 14, 25, 15, 15, 21, 28, 16, 13, 15, 14, 15, 16, 15, 28, 16, 16, 16, 12, 15, 16, 20, 20, 15, 14, 16, 26, 15, 15, 15, 6, 16, 16, 12, 15, 16, 15, 16, 0, 15, 15, 14, 15, 15, 16, 15, 14, 5, 16, 25, 16, 15, 22, 16, 20, 16, 16, 20, 12, 16, 16, 24, 14, 12, 25, 15, 16, 20, 20, 15, 16, 15, 16, 15, 16, 16, 12, 12, 15, 3, 16, 18, 16, 16, 16, 14, 18, 15, 15, 16, 28, 15, 15, 16, 16, 21, 14, 16, 15, 14, 14, 16, 12, 15, 3, 28, 14, 14, 15, 15, 15, 16, 16, 12, 19, 16, 6, 20, 16, 21, 15, 16, 15, 15, 15, 20, 15, 18, 15, 14, 15, 15, 15, 0, 16, 12, 16, 15, 15, 15, 12, 16, 14, 15, 20, 15, 6, 16, 15, 15, 16, 15, 16, 19, 16, 16, 20, 16, 6, 15, 20, 18, 15, 28, 28, 16, 14, 28, 15, 15, 16, 15, 16, 16, 13, 15, 17, 21, 16, 16, 16, 16, 16, 16, 15, 15, 16, 16, 16, 15, 20, 16, 15, 16, 15, 15, 16, 2, 5, 5, 17, 21, 4, 5, 17, 4, 5, 14, 24, 17, 28, 0, 17, 4, 17, 25, 23, 17, 17, 4, 13, 17, 17, 5, 19, 5, 18, 17, 21, 17, 17, 17, 4, 17, 20, 17, 17, 18, 17, 17, 21, 17, 23, 5, 17, 17, 17, 17, 21, 28, 6, 17, 15, 5, 17, 17, 17, 21, 5, 23, 25, 17, 15, 5, 25, 5, 17, 5, 5, 17, 23, 27, 25, 27, 17, 0, 17, 17, 17, 17, 6, 28, 13, 17, 17, 28, 12, 0, 25, 17, 17, 17, 4, 14, 17, 13, 11, 17, 5, 28, 13, 15, 17, 18, 5, 5, 19, 17, 21, 5, 17, 28, 0, 17, 26, 6, 17, 17, 21, 17, 4, 17, 0, 21, 17, 5, 17, 17, 27, 17, 26, 17, 17, 21, 17, 21, 3, 4, 23, 5, 28, 17, 17, 4, 11, 17, 17, 0, 14, 12, 25, 23, 23, 27, 4, 21, 17, 17, 17, 5, 14, 4, 27, 4, 17, 17, 14, 5, 25, 0, 23, 17, 6, 17, 6, 20, 13, 17, 4, 17, 17, 13, 25, 17, 6, 13, 17, 17, 17, 4, 17, 5, 17, 4, 17, 17, 17, 27, 23, 21, 28, 18, 3, 4, 25, 6, 17, 13, 14, 17, 0, 13, 5, 4, 5, 13, 13, 5, 4, 25, 17, 17, 25, 5, 17, 5, 17, 17, 27, 2, 17, 17, 28, 17, 21, 0, 13, 0, 13, 18, 4, 17, 17, 17, 17, 17, 7, 5, 9, 20, 17, 4, 4, 5, 17, 26, 17, 17, 14, 0, 17, 13, 17, 16, 0, 21, 17, 5, 17, 11, 17, 15, 17, 21, 23, 19, 21, 17, 17, 28, 5, 28, 23, 17, 12, 21, 17, 17, 25, 26, 17, 17, 21, 4, 4, 0, 6, 17, 5, 14, 0, 12, 22, 16, 18, 17, 17, 25, 12, 13, 17, 5, 17, 14, 4, 17, 17, 17, 27, 17, 12, 17, 17, 5, 17, 17, 17, 17, 13, 17, 14, 12, 17, 15, 18, 17, 4, 5, 17, 5, 17, 17, 4, 21, 17, 5, 14, 14, 18, 5, 0, 18, 18, 19, 18, 5, 15, 18, 17, 17, 18, 18, 18, 18, 18, 18, 16, 16, 18, 6, 15, 21, 6, 18, 9, 0, 15, 18, 6, 18, 1, 5, 27, 19, 6, 6, 0, 8, 17, 0, 18, 4, 6, 17, 23, 20, 18, 18, 0, 0, 17, 17, 28, 19, 15, 21, 0, 16, 14, 6, 18, 18, 18, 19, 18, 15, 18, 18, 16, 17, 18, 18, 18, 18, 18, 18, 20, 18, 17, 20, 6, 18, 16, 18, 20, 18, 3, 19, 0, 6, 6, 6, 6, 6, 0, 15, 18, 17, 25, 3, 17, 18, 6, 18, 18, 4, 18, 14, 6, 18, 0, 22, 18, 18, 0, 20, 18, 17, 16, 19, 18, 6, 0, 17, 18, 18, 26, 0, 14, 20, 18, 18, 6, 21, 18, 6, 28, 18, 6, 18, 25, 12, 18, 14, 20, 17, 20, 0, 16, 4, 16, 15, 12, 18, 6, 19, 17, 6, 18, 6, 6, 22, 18, 5, 0, 17, 16, 18, 27, 6, 18, 24, 14, 18, 0, 14, 28, 6, 17, 3, 18, 12, 18, 22, 18, 20, 6, 17, 14, 19, 28, 6, 18, 17, 6, 16, 18, 18, 20, 6, 14, 5, 18, 18, 6, 6, 6, 25, 4, 10, 6, 17, 0, 12, 18, 14, 22, 8, 19, 18, 18, 6, 18, 0, 18, 19, 18, 15, 24, 18, 10, 17, 6, 6, 19, 17, 15, 14, 18, 6, 18, 17, 17, 18, 18, 16, 6, 6, 17, 6, 20, 18, 0, 18, 19, 3, 18, 6, 18, 5, 15, 26, 17, 18, 19, 14, 14, 18, 19, 4, 24, 25, 6, 12, 6, 22, 17, 18, 24, 0, 18, 0, 20, 17, 6, 12, 17, 16, 19, 0, 6, 25, 14, 15, 15, 18, 5, 4, 14, 6, 18, 18, 19, 15, 6, 17, 18, 19, 0, 6, 0, 20, 6, 6, 18, 18, 18, 19, 6, 12, 18, 8, 6, 0, 6, 19, 18, 18, 14, 17, 22, 18, 27, 18, 12, 6, 17, 10, 18, 25, 19, 19, 6, 13, 18, 14, 7, 16, 3, 6, 19, 18, 19, 19, 19, 15, 14, 14, 24, 19, 19, 6, 3, 6, 28, 19, 19, 19, 19, 19, 16, 19, 14, 20, 20, 24, 14, 25, 6, 6, 19, 14, 19, 19, 24, 12, 15, 24, 19, 15, 19, 19, 19, 28, 19, 19, 19, 18, 19, 12, 19, 22, 19, 16, 14, 18, 19, 24, 19, 17, 19, 15, 7, 12, 19, 14, 19, 6, 19, 20, 24, 6, 17, 6, 28, 21, 5, 12, 6, 18, 14, 14, 19, 19, 19, 27, 24, 19, 18, 22, 20, 12, 19, 6, 20, 19, 20, 26, 19, 6, 25, 16, 20, 18, 19, 20, 19, 12, 18, 14, 14, 8, 11, 12, 6, 6, 19, 19, 0, 19, 18, 12, 12, 19, 14, 6, 14, 12, 6, 14, 20, 24, 13, 18, 19, 20, 16, 19, 22, 16, 13, 11, 18, 19, 20, 19, 14, 19, 19, 19, 19, 15, 20, 20, 24, 24, 14, 25, 18, 20, 19, 19, 19, 19, 6, 6, 19, 14, 19, 18, 6, 14, 18, 19, 24, 19, 16, 19, 19, 7, 19, 14, 0, 20, 19, 19, 10, 19, 18, 20, 17, 12, 14, 10, 19, 12, 6, 19, 14, 6, 19, 19, 19, 19, 19, 20, 20, 18, 15, 14, 19, 24, 20, 6, 15, 15, 20, 2, 20, 14, 18, 19, 19, 19, 19, 11, 19, 19, 19, 2, 14, 19, 15, 14, 20, 19, 3, 11, 19, 3, 19, 19, 14, 11, 19, 19, 19, 19, 18, 6, 19, 19, 19, 0, 18, 18, 14, 22, 19, 18, 19, 24, 12, 8, 5, 19, 18, 19, 14, 19, 24, 19, 26, 14, 13, 2, 23, 16, 19, 19, 19, 6, 24, 19, 19, 6, 6, 24, 14, 19, 14, 14, 16, 19, 2, 7, 6, 24, 18, 19, 14, 19, 27, 5, 19, 20, 24, 20, 22, 20, 22, 24, 27, 14, 3, 6, 19, 15, 4, 16, 28, 19, 19, 18, 18, 19, 16, 24, 20, 19, 20, 14, 15, 6, 28, 6, 19, 24, 2, 20, 20, 18, 26, 6, 20, 20, 3, 20, 20, 20, 21, 11, 20, 15, 3, 20, 14, 6, 25, 28, 19, 20, 20, 20, 14, 28, 12, 18, 12, 15, 20, 20, 19, 21, 17, 21, 25, 20, 19, 20, 28, 16, 20, 28, 14, 20, 19, 22, 20, 20, 16, 25, 22, 12, 20, 12, 22, 20, 20, 20, 25, 16, 14, 18, 16, 18, 12, 18, 18, 20, 20, 2, 20, 19, 22, 20, 20, 20, 14, 20, 20, 19, 20, 19, 28, 18, 20, 20, 21, 16, 19, 25, 20, 12, 16, 20, 20, 20, 20, 22, 20, 15, 19, 3, 19, 21, 19, 19, 20, 20, 20, 19, 20, 16, 20, 20, 20, 21, 20, 19, 20, 20, 12, 20, 14, 20, 20, 25, 20, 16, 20, 12, 22, 22, 20, 28, 28, 20, 16, 20, 14, 22, 20, 18, 12, 17, 20, 14, 25, 19, 22, 19, 25, 20, 19, 16, 19, 12, 24, 19, 19, 25, 20, 18, 7, 20, 14, 20, 14, 28, 19, 20, 20, 20, 21, 20, 20, 20, 20, 20, 20, 20, 19, 10, 16, 20, 20, 12, 19, 20, 6, 6, 20, 12, 25, 22, 20, 20, 20, 22, 20, 19, 20, 26, 25, 28, 12, 20, 20, 20, 20, 20, 12, 13, 20, 5, 17, 20, 12, 18, 6, 19, 12, 20, 28, 2, 20, 20, 20, 20, 6, 20, 16, 25, 25, 20, 24, 12, 16, 25, 20, 20, 20, 19, 20, 20, 19, 20, 20, 20, 20, 20, 28, 20, 12, 22, 20, 20, 22, 5, 20, 12, 20, 25, 18, 14, 6, 13, 20, 13, 6, 25, 21, 16, 20, 1, 20, 14, 24, 20, 16, 20, 28, 27, 20, 19, 20, 20, 20, 20, 19, 20, 12, 25, 25, 20, 14, 20, 20, 15, 12, 19, 20, 12, 20, 20, 20, 20, 19, 19, 16, 20, 20, 20, 24, 12, 0, 14, 27, 20, 20, 19, 20, 12, 28, 20, 20, 20, 20, 16, 14, 19, 25, 17, 0, 6, 28, 20, 28, 20, 20, 20, 16, 16, 20, 10, 28, 26, 22, 2, 7, 6, 21, 28, 21, 21, 21, 18, 3, 21, 14, 21, 20, 28, 21, 21, 28, 28, 28, 12, 21, 17, 21, 17, 21, 3, 21, 16, 20, 20, 20, 21, 21, 17, 21, 28, 2, 4, 27, 12, 12, 21, 21, 21, 5, 28, 21, 28, 21, 2, 21, 20, 21, 17, 25, 28, 21, 24, 14, 22, 12, 12, 25, 21, 21, 21, 28, 21, 20, 21, 28, 21, 21, 21, 17, 21, 21, 21, 28, 20, 24, 21, 17, 20, 21, 21, 21, 20, 21, 17, 21, 18, 22, 21, 28, 2, 0, 2, 21, 21, 25, 21, 21, 12, 21, 13, 21, 20, 26, 21, 25, 16, 21, 21, 21, 21, 12, 28, 21, 21, 21, 21, 21, 21, 28, 28, 25, 28, 21, 28, 21, 3, 28, 21, 21, 25, 21, 21, 22, 17, 28, 21, 17, 21, 21, 21, 21, 20, 20, 21, 6, 13, 21, 17, 28, 21, 21, 28, 28, 21, 14, 2, 3, 4, 21, 21, 2, 28, 27, 28, 19, 21, 12, 21, 3, 27, 21, 13, 21, 28, 6, 17, 16, 21, 20, 21, 21, 21, 21, 16, 28, 21, 21, 3, 21, 20, 24, 21, 21, 21, 5, 27, 21, 3, 21, 6, 21, 6, 22, 21, 28, 22, 21, 21, 21, 21, 21, 21, 24, 13, 21, 24, 21, 17, 21, 28, 15, 21, 21, 21, 21, 20, 21, 7, 14, 21, 16, 6, 17, 21, 17, 21, 21, 20, 0, 16, 3, 21, 25, 21, 21, 21, 21, 21, 21, 3, 21, 12, 24, 0, 5, 21, 22, 14, 21, 21, 21, 6, 25, 3, 17, 22, 21, 2, 21, 25, 26, 14, 28, 28, 3, 21, 21, 21, 21, 21, 3, 17, 17, 27, 25, 3, 21, 21, 14, 17, 22, 21, 28, 21, 21, 14, 5, 16, 19, 22, 17, 21, 22, 28, 21, 2, 21, 2, 21, 21, 21, 22, 14, 21, 21, 16, 5, 21, 28, 21, 21, 21, 2, 21, 27, 28, 22, 25, 21, 17, 18, 17, 21, 2, 2, 25, 21, 20, 22, 19, 3, 22, 22, 22, 22, 22, 22, 22, 15, 20, 22, 22, 22, 28, 0, 22, 22, 15, 22, 22, 9, 24, 22, 22, 16, 22, 22, 22, 0, 22, 22, 28, 22, 12, 0, 22, 0, 6, 22, 22, 18, 22, 25, 0, 12, 22, 22, 22, 22, 15, 0, 22, 22, 24, 22, 3, 22, 24, 22, 3, 28, 22, 22, 22, 0, 16, 21, 19, 7, 22, 22, 22, 22, 22, 20, 22, 3, 22, 22, 15, 22, 22, 12, 12, 22, 22, 22, 22, 22, 22, 22, 22, 12, 22, 22, 22, 0, 22, 0, 22, 22, 22, 0, 22, 22, 22, 22, 19, 0, 28, 24, 22, 22, 28, 22, 22, 22, 22, 22, 22, 22, 11, 12, 10, 22, 22, 22, 22, 6, 22, 22, 22, 22, 21, 22, 3, 20, 17, 22, 22, 22, 10, 0, 13, 28, 28, 12, 22, 22, 22, 28, 22, 22, 21, 22, 22, 19, 22, 22, 22, 9, 22, 22, 22, 22, 22, 22, 22, 0, 22, 22, 24, 12, 22, 22, 22, 15, 21, 24, 12, 24, 22, 22, 19, 0, 22, 22, 12, 20, 3, 22, 22, 24, 22, 28, 22, 6, 22, 22, 9, 22, 9, 22, 0, 22, 22, 17, 22, 3, 14, 22, 10, 22, 28, 24, 6, 22, 22, 22, 12, 0, 22, 22, 22, 22, 22, 24, 22, 22, 19, 3, 19, 22, 28, 22, 12, 7, 15, 12, 7, 20, 22, 9, 22, 0, 22, 22, 25, 22, 22, 24, 22, 22, 22, 22, 21, 22, 25, 0, 24, 22, 10, 22, 22, 22, 22, 3, 12, 0, 0, 22, 22, 22, 22, 4, 22, 24, 3, 22, 22, 18, 3, 7, 1, 22, 12, 22, 22, 22, 22, 6, 22, 22, 7, 18, 22, 7, 12, 7, 28, 21, 0, 22, 0, 20, 22, 22, 3, 22, 17, 22, 0, 22, 22, 19, 22, 22, 22, 28, 22, 22, 22, 22, 22, 9, 28, 0, 16, 24, 0, 22, 22, 22, 22, 21, 22, 22, 22, 21, 22, 22, 22, 3, 22, 22, 3, 22, 22, 14, 4, 18, 23, 11, 5, 23, 23, 11, 23, 23, 5, 23, 0, 25, 11, 5, 17, 27, 13, 4, 23, 23, 6, 4, 23, 23, 23, 17, 24, 23, 13, 1, 5, 5, 18, 23, 13, 4, 25, 23, 17, 23, 23, 23, 23, 5, 23, 23, 18, 13, 23, 17, 1, 25, 25, 9, 23, 17, 11, 23, 3, 5, 17, 23, 4, 11, 0, 1, 28, 23, 23, 17, 23, 11, 23, 17, 23, 11, 23, 23, 5, 13, 23, 17, 17, 25, 14, 23, 11, 13, 23, 23, 23, 25, 15, 23, 15, 5, 5, 27, 23, 4, 23, 13, 23, 17, 25, 28, 23, 23, 23, 25, 18, 23, 25, 23, 23, 13, 13, 5, 23, 23, 17, 25, 17, 11, 13, 23, 23, 23, 5, 23, 23, 25, 23, 17, 17, 13, 23, 1, 23, 25, 17, 17, 11, 23, 17, 23, 23, 23, 23, 23, 23, 17, 23, 5, 4, 22, 14, 23, 7, 21, 26, 23, 5, 11, 26, 23, 27, 13, 23, 27, 13, 14, 0, 27, 23, 11, 23, 23, 26, 14, 5, 17, 17, 4, 15, 13, 23, 23, 4, 4, 11, 23, 23, 23, 23, 23, 17, 25, 5, 13, 17, 11, 27, 13, 17, 13, 23, 17, 28, 23, 17, 5, 14, 23, 23, 25, 25, 13, 13, 15, 23, 17, 1, 23, 23, 27, 11, 13, 23, 17, 23, 23, 25, 13, 25, 5, 23, 23, 23, 23, 27, 23, 23, 24, 23, 6, 5, 23, 0, 23, 23, 4, 23, 23, 17, 11, 23, 23, 23, 23, 27, 25, 4, 23, 23, 23, 13, 18, 23, 1, 13, 17, 0, 13, 15, 23, 23, 11, 23, 13, 13, 27, 23, 23, 5, 23, 13, 23, 13, 23, 23, 23, 17, 23, 17, 27, 23, 0, 25, 23, 23, 4, 11, 23, 23, 17, 23, 23, 4, 11, 23, 13, 17, 13, 25, 13, 13, 4, 11, 23, 27, 23, 23, 17, 17, 23, 13, 13, 13, 11, 21, 11, 5, 23, 13, 13, 23, 13, 23, 17, 23, 23, 13, 2, 3, 11, 24, 20, 12, 6, 24, 3, 19, 24, 24, 11, 3, 24, 1, 27, 2, 24, 7, 24, 3, 8, 24, 27, 22, 24, 2, 24, 6, 24, 2, 0, 3, 6, 24, 24, 22, 22, 24, 7, 20, 24, 2, 3, 24, 24, 12, 20, 24, 18, 8, 24, 24, 24, 12, 24, 24, 24, 3, 12, 19, 18, 24, 11, 24, 11, 19, 24, 3, 19, 3, 24, 11, 8, 21, 3, 18, 21, 24, 5, 24, 24, 16, 14, 2, 8, 24, 7, 24, 8, 19, 15, 22, 12, 3, 18, 24, 3, 15, 24, 18, 12, 2, 3, 22, 24, 24, 24, 24, 24, 24, 2, 24, 24, 2, 24, 24, 24, 2, 24, 11, 25, 2, 3, 28, 12, 24, 11, 24, 24, 24, 24, 20, 21, 22, 24, 3, 24, 20, 24, 2, 24, 8, 24, 0, 24, 2, 24, 22, 27, 24, 11, 24, 24, 24, 2, 24, 24, 28, 8, 24, 2, 24, 2, 2, 24, 24, 27, 18, 24, 24, 27, 20, 24, 21, 3, 20, 3, 16, 6, 24, 1, 24, 24, 18, 24, 24, 2, 22, 24, 2, 10, 3, 2, 24, 28, 1, 24, 6, 0, 24, 24, 10, 3, 24, 24, 24, 2, 24, 12, 8, 3, 11, 12, 24, 24, 24, 19, 6, 19, 24, 8, 8, 24, 3, 24, 24, 24, 24, 24, 2, 19, 12, 11, 2, 11, 24, 3, 24, 2, 6, 2, 24, 24, 27, 24, 2, 22, 22, 3, 25, 2, 24, 24, 5, 24, 24, 24, 2, 24, 13, 24, 24, 2, 24, 24, 24, 24, 20, 22, 2, 3, 16, 19, 12, 24, 10, 27, 24, 24, 24, 12, 24, 24, 3, 24, 12, 3, 24, 2, 24, 12, 24, 3, 7, 24, 24, 24, 3, 11, 6, 24, 3, 11, 15, 24, 5, 8, 24, 2, 22, 20, 22, 8, 20, 24, 3, 14, 18, 24, 2, 6, 18, 3, 2, 6, 2, 2, 24, 2, 28, 12, 24, 21, 11, 24, 24, 24, 3, 3, 24, 10, 24, 24, 24, 24, 3, 2, 24, 24, 25, 24, 24, 19, 24, 17, 25, 25, 16, 26, 25, 13, 25, 22, 25, 21, 17, 4, 20, 23, 0, 25, 25, 15, 25, 25, 25, 25, 4, 21, 13, 23, 25, 25, 18, 25, 25, 21, 13, 23, 11, 25, 0, 25, 17, 14, 23, 14, 14, 2, 5, 15, 12, 25, 15, 25, 7, 14, 25, 20, 11, 5, 25, 5, 25, 25, 28, 4, 25, 13, 21, 17, 17, 25, 25, 4, 25, 25, 13, 18, 12, 25, 3, 4, 16, 13, 25, 15, 25, 5, 25, 23, 25, 25, 4, 23, 4, 4, 25, 27, 0, 0, 15, 27, 25, 8, 25, 4, 2, 25, 20, 23, 21, 25, 25, 4, 25, 27, 26, 15, 25, 27, 25, 25, 25, 16, 25, 21, 25, 25, 25, 25, 4, 25, 0, 20, 13, 4, 15, 25, 25, 23, 25, 25, 6, 4, 25, 23, 25, 0, 25, 25, 14, 25, 25, 25, 21, 17, 25, 17, 15, 28, 25, 25, 18, 27, 0, 21, 16, 2, 25, 17, 25, 5, 25, 25, 21, 18, 4, 25, 25, 25, 25, 25, 4, 26, 13, 25, 21, 3, 23, 4, 25, 27, 25, 25, 25, 4, 25, 14, 25, 25, 13, 25, 16, 25, 6, 25, 25, 8, 25, 21, 4, 23, 25, 25, 25, 26, 25, 4, 25, 25, 25, 25, 13, 25, 13, 0, 4, 21, 25, 25, 13, 27, 27, 13, 25, 16, 25, 14, 27, 25, 25, 0, 25, 13, 4, 28, 25, 25, 25, 21, 4, 25, 26, 13, 14, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 1, 25, 14, 25, 25, 25, 25, 25, 20, 25, 25, 25, 25, 25, 15, 1, 17, 25, 25, 4, 25, 25, 13, 26, 25, 17, 25, 13, 13, 17, 26, 25, 16, 25, 25, 13, 25, 25, 25, 25, 25, 2, 23, 15, 25, 25, 25, 25, 25, 25, 25, 25, 25, 17, 14, 25, 13, 4, 11, 15, 25, 14, 23, 25, 25, 25, 4, 25, 25, 17, 25, 17, 1, 4, 28, 25, 25, 25, 2, 13, 25, 25, 25, 16, 19, 28, 26, 26, 23, 26, 19, 26, 26, 7, 26, 25, 26, 25, 26, 9, 16, 26, 26, 26, 14, 26, 26, 26, 26, 25, 19, 9, 26, 26, 26, 9, 26, 28, 26, 25, 26, 26, 25, 26, 23, 26, 18, 26, 26, 26, 9, 9, 26, 20, 26, 26, 25, 28, 5, 18, 28, 25, 9, 20, 26, 26, 26, 28, 25, 26, 26, 5, 25, 26, 22, 13, 26, 26, 26, 10, 26, 27, 26, 26, 26, 26, 26, 26, 12, 26, 26, 26, 26, 26, 26, 0, 26, 9, 26, 26, 13, 26, 20, 26, 10, 26, 13, 22, 26, 26, 26, 9, 18, 5, 13, 26, 26, 20, 26, 28, 26, 26, 26, 26, 14, 26, 23, 28, 19, 26, 25, 26, 26, 26, 25, 26, 26, 26, 28, 10, 26, 26, 22, 17, 17, 26, 4, 26, 25, 26, 26, 26, 26, 7, 26, 17, 26, 26, 25, 26, 26, 26, 26, 28, 28, 26, 18, 20, 23, 28, 26, 26, 28, 26, 9, 26, 12, 26, 28, 26, 27, 27, 27, 27, 27, 27, 11, 5, 5, 1, 27, 13, 17, 17, 11, 1, 27, 1, 23, 23, 17, 1, 22, 27, 21, 27, 27, 27, 28, 4, 27, 27, 22, 11, 1, 11, 27, 28, 27, 1, 27, 23, 27, 1, 17, 22, 27, 1, 27, 27, 3, 17, 27, 27, 27, 27, 27, 1, 11, 13, 5, 27, 1, 11, 17, 25, 27, 21, 27, 1, 27, 27, 13, 27, 21, 1, 27, 27, 27, 27, 25, 1, 27, 27, 27, 1, 27, 4, 4, 27, 27, 6, 18, 27, 23, 27, 11, 17, 27, 24, 27, 27, 27, 12, 1, 1, 27, 23, 17, 27, 1, 27, 17, 27, 1, 27, 27, 27, 27, 27, 19, 25, 23, 27, 27, 11, 1, 27, 4, 25, 27, 27, 27, 27, 12, 27, 27, 27, 27, 27, 5, 27, 27, 1, 27, 27, 27, 4, 27, 27, 4, 25, 24, 27, 23, 27, 27, 27, 27, 23, 8, 27, 24, 27, 27, 27, 27, 1, 27, 21, 27, 27, 27, 27, 27, 27, 27, 27, 25, 23, 27, 27, 27, 21, 27, 9, 27, 17, 4, 17, 17, 27, 4, 27, 1, 13, 19, 10, 3, 27, 1, 27, 27, 27, 27, 27, 27, 4, 1, 27, 5, 27, 27, 27, 27, 27, 27, 27, 27, 19, 27, 22, 2, 27, 27, 27, 15, 1, 27, 1, 27, 5, 27, 11, 27, 27, 27, 27, 4, 27, 27, 27, 11, 27, 21, 2, 27, 23, 1, 27, 27, 24, 12, 27, 27, 4, 27, 27, 27, 13, 27, 17, 18, 13, 27, 27, 4, 27, 4, 1, 4, 27, 27, 4, 27, 27, 27, 2, 27, 27, 24, 4, 1, 27, 27, 25, 27, 1, 4, 27, 4, 17, 27, 28, 27, 27, 27, 1, 27, 1, 1, 11, 27, 27, 27, 27, 27, 21, 1, 27, 27, 1, 21, 4, 4, 27, 18, 27, 1, 27, 27, 27, 27, 11, 11, 27, 27, 27, 27, 27, 27, 27, 23, 27, 27, 1, 23, 27, 27, 27, 27, 27, 27, 25, 6, 11, 0, 28, 6, 28, 28, 28, 16, 28, 28, 28, 28, 7, 28, 25, 21, 20, 28, 28, 23, 28, 5, 25, 17, 20, 28, 28, 24, 28, 0, 28, 28, 10, 3, 28, 27, 22, 28, 20, 14, 28, 28, 28, 5, 21, 9, 0, 3, 28, 8, 28, 6, 28, 23, 28, 28, 28, 9, 28, 11, 1, 0, 0, 25, 0, 28, 21, 22, 28, 28, 22, 17, 2, 28, 16, 7, 22, 6, 27, 28, 12, 3, 28, 28, 4, 21, 20, 28, 28, 28, 28, 28, 28, 20, 28, 22, 21, 25, 25, 8, 20, 28, 28, 28, 28, 28, 12, 28, 28, 22, 25, 21, 21, 26, 28, 28, 28, 0, 21, 28, 28, 28, 17, 17, 28, 3, 1, 16, 28, 0, 28, 21, 28, 17, 24, 8, 12, 21, 0, 25, 3, 18, 26, 28, 28, 8, 9, 28, 2, 4, 28, 28, 17, 4, 7, 28, 28, 27, 1, 3, 12, 18, 28, 2, 28, 28, 28, 26, 28, 24, 3, 28, 28, 27, 11, 28, 28, 3, 25, 28, 12, 28, 28, 28, 28, 28, 20, 28, 3, 28, 3, 28, 28, 28, 26, 10, 28, 8, 21, 4, 28, 22, 28, 28, 28, 21, 28, 18, 28, 15, 8, 28, 28, 25, 28, 0, 21, 19, 20, 0, 13, 22, 3, 17, 28, 24, 20, 24, 4, 3, 1, 25, 28, 28, 27, 28, 28, 17, 22, 21, 28, 17, 8, 28, 28, 28, 16, 28, 21, 19, 3, 28, 10, 28, 0, 28, 28, 28, 2, 28, 6, 5, 28, 1, 21, 20, 6, 21, 23, 28, 11, 28, 20, 28, 21, 28, 26, 28, 28, 28, 10, 28, 28, 1, 10, 28, 28, 18, 21, 21, 28, 26, 28, 28, 6, 3, 22, 10, 24, 28, 7, 28, 28, 28, 22, 25, 26, 20, 28, 22, 28, 25, 27, 23, 28, 28, 24, 2, 28, 0, 28, 28, 28, 21, 3, 28, 28, 22, 28, 28, 28, 25, 18, 28, 7, 12, 22, 17, 28, 16, 24, 3, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation : \n",
        "#confusion matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "#print(confusion_matrix(test, pred))\n",
        "#plot_confusion_matrix(model,testX, testy)\n",
        "#plt.show()\n",
        "\n",
        "#precision\n",
        "from sklearn.metrics import precision_score\n",
        "\n",
        "print(f\"Precision Score of the classifier is: {precision_score(test, pred,average='macro')}\")\n",
        "\n",
        "\n",
        "# recall\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "# Calculating the recall score of classifier\n",
        "print(f\"Recall Score of the classifier is: {recall_score(test, pred,average='macro')}\")"
      ],
      "metadata": {
        "id": "Y-mhwNPQlHoe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "339cdacb-cad7-4140-b4f6-069137e3b4b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision Score of the classifier is: 0.947387242856312\n",
            "Recall Score of the classifier is: 0.9464504536485254\n"
          ]
        }
      ]
    }
  ]
}
